{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Sign Detection and Classification\n",
    "\n",
    "This work focuses on the development of a program for the automatic detection and classification of a subset of traffic signs, namely traffic lights, stop signs, speed limit signs and crosswalk signs, using a Deep Learning approach.\n",
    "\n",
    "The first step is to import all the necessaries libraries that will be used in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bbox import BBox2D\n",
    "import xml.etree.ElementTree as ET\n",
    "from math import prod\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Now we must define the hyperparameters to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    model_name = \"vgg16\"\n",
    "    classes = {'trafficlight': 0, 'stop': 1, 'speedlimit': 2, 'crosswalk': 3}\n",
    "    data_folder = './data'\n",
    "    annotations_folder = './data/annotations/'\n",
    "    images_folder = './data/images/'\n",
    "    images_size = 300\n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-3\n",
    "    batch_size = 16\n",
    "    num_filters = 32\n",
    "    kernel_size = 5\n",
    "    pool_size = 2\n",
    "    padding = 0\n",
    "    stride = 1\n",
    "    num_workers = 2 \n",
    "    device =  \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Classes\n",
    "\n",
    "These are the Dataset classes that we created, one for the advanced version (multilabel) and the other for the other versions (basic and intermediate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = pd.DataFrame(images, columns=['image_name'])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(f'{Config.images_folder}{self.images.iloc[idx, 0]}.png')\n",
    "        try:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        except:\n",
    "            print(f'Error reading image {self.images.iloc[idx, 0]}.png')\n",
    "            return None\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        tree = ET.parse(Config.annotations_folder + f'{self.images.iloc[idx, 0]}.xml')\n",
    "        correct_labels = [movie.text for movie in tree.getroot().iter('name')]\n",
    "        objects = [obj for obj in tree.getroot().iter('object')]\n",
    "        objects = [(obj.find('name').text, [int(obj.find('bndbox').find('xmin').text), int(obj.find('bndbox').find('ymin').text), int(obj.find('bndbox').find('xmax').text), int(obj.find('bndbox').find('ymax').text)]) for obj in objects]\n",
    "\n",
    "        labels = []\n",
    "\n",
    "        greater_area = 0\n",
    "        label = None\n",
    "        if correct_labels:\n",
    "            for obj in objects:\n",
    "                box = BBox2D(obj[1])\n",
    "                area = box.height * box.width\n",
    "                greater_area = area if area > greater_area else greater_area\n",
    "                label = obj[0] if (area > greater_area or label is None) else label\n",
    "        \n",
    "        labels = Config.classes[label]\n",
    "        labels = np.asarray(labels)\n",
    "        labels = torch.from_numpy(labels.astype('long'))\n",
    "\n",
    "        result = {\n",
    "            'name': self.images.iloc[idx, 0],\n",
    "            'image': image.float(),\n",
    "            'labels': labels.float()\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "class ImageMultiLabelDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = pd.DataFrame(images, columns=['image_name'])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(f'{Config.images_folder}{self.images.iloc[idx, 0]}.png')\n",
    "        try:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        except:\n",
    "            print(f'Error reading image {self.images.iloc[idx, 0]}.png')\n",
    "            return None\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        tree = ET.parse(Config.annotations_folder + f'{self.images.iloc[idx, 0]}.xml')\n",
    "        correct_labels = [movie.text for movie in tree.getroot().iter('name')]\n",
    "\n",
    "        labels = []\n",
    "        for cl in Config.classes.keys():\n",
    "            labels.append(1) if cl in correct_labels else labels.append(0)\n",
    "\n",
    "        labels = np.asarray(labels)\n",
    "        labels = torch.from_numpy(labels.astype('long'))\n",
    "\n",
    "        result = {\n",
    "            'name': self.images.iloc[idx, 0],\n",
    "            'image': image.float(),\n",
    "            'labels': labels.float()\n",
    "        }\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n",
    "\n",
    "Define some utils functions mainly for the data display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    @staticmethod\n",
    "    def calculate_output_size(input_size):\n",
    "        return (input_size - Config.kernel_size + 2*Config.padding) / Config.stride + 1\n",
    "\n",
    "    @staticmethod\n",
    "    def learning_curve_graph(train_history, val_history):\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.title('Cross Entropy Loss')\n",
    "        plt.plot(train_history['loss'], label='train')\n",
    "        plt.plot(val_history['loss'], label='val')\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.title('Classification Accuracy')\n",
    "        plt.plot(train_history['accuracy'], label='train')\n",
    "        plt.plot(val_history['accuracy'], label='val')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def display_predictions(model, data, multilabel=False):    \n",
    "        plt.figure(figsize=(15, 15))\n",
    "\n",
    "        for ind, data in enumerate(tqdm(data)):\n",
    "            if (ind % 100 == 0) and (ind != 0): plt.show()\n",
    "            ind = ind % 100\n",
    "\n",
    "            ######\n",
    "            # Save this somewhere else\n",
    "            ######\n",
    "            inputs, labels = data['image'], data['labels']\n",
    "            labels = labels.type(torch.LongTensor) \n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            \n",
    "            pred = model(inputs)\n",
    "            probs = F.softmax(pred, dim=1)\n",
    "            final_pred = torch.argmax(probs, dim=1)\n",
    "\n",
    "            if multilabel:\n",
    "                threshold = 0.3\n",
    "                final_pred = np.array([[1 if i > threshold else 0 for i in j] for j in probs])\n",
    "                final_pred = torch.from_numpy(final_pred)\n",
    "\n",
    "            inputs = inputs[0].cpu()\n",
    "            \n",
    "            ######\n",
    "            # Save this somewhere else\n",
    "            ######\n",
    "            if multilabel:\n",
    "                plt.subplot(10, 10, ind + 1)\n",
    "                plt.axis(\"off\")\n",
    "                labels = [idx for idx, label in enumerate(labels[0]) if label.item() == 1]\n",
    "                preds = [idx for idx, pred in enumerate(final_pred[0]) if pred.item() == 1]\n",
    "                for i, label in enumerate(labels):\n",
    "                    plt.text(50*i, -1, label, fontsize=14, color='green') # correct\n",
    "                for j, pred in enumerate(preds):\n",
    "                    plt.text(50*(i+1) + 50*j, -1, pred, fontsize=14, color='red') # predicted\n",
    "            else:\n",
    "                plt.subplot(10, 10, ind + 1)\n",
    "                plt.axis(\"off\")\n",
    "                plt.text(0, -1, labels[0].item(), fontsize=14, color='green') # correct\n",
    "                plt.text(100, -1, final_pred[0].item(), fontsize=14, color='red') # predicted\n",
    "\n",
    "            plt.imshow(inputs.permute(1, 2, 0).numpy())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test methods\n",
    "\n",
    "The main methods of our program, which contain the training and testing processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Iterator:\n",
    "    @staticmethod\n",
    "    def epoch_iterator(dataloader, model, loss_function, optimizer=None, is_train=True, multilabel=False):\n",
    "        if is_train: assert optimizer is not None, 'When training, please provide an optimizer.'\n",
    "        \n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        if is_train:\n",
    "            model.train() # put model in train mode\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        preds = []\n",
    "        labels = []\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            for batch, data in enumerate(tqdm(dataloader)):\n",
    "                data['labels'] = data['labels'].type(torch.LongTensor) if not multilabel else data['labels']\n",
    "                X, y = data['image'].to(Config.device), data['labels'].to(Config.device)\n",
    "\n",
    "                # Compute prediction error\n",
    "                pred = model(X)\n",
    "                m = nn.Sigmoid()\n",
    "\n",
    "                loss = loss_function(m(pred), y)\n",
    "\n",
    "                # Backpropagation\n",
    "                if is_train:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Save training metrics\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                probs = F.softmax(pred, dim=1)\n",
    "                final_pred = torch.argmax(probs, dim=1)\n",
    "\n",
    "                if multilabel:\n",
    "                    threshold = 0.25\n",
    "                    final_pred = np.array([[1.0 if i > threshold else 0.0 for i in j] for j in probs])\n",
    "                    final_pred = torch.from_numpy(final_pred).to(Config.device)\n",
    "                \n",
    "                preds.extend(final_pred.cpu().numpy())\n",
    "                labels.extend(y.cpu().numpy())\n",
    "\n",
    "        return total_loss / num_batches, accuracy_score(labels, preds)\n",
    "\n",
    "    @staticmethod\n",
    "    def train(model, train_dataloader, validation_dataloader, loss_fn, multilabel=False):\n",
    "        train_history = {'loss': [], 'accuracy': []}\n",
    "        val_history = {'loss': [], 'accuracy': []}\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) if multilabel else torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        print(\"\\nStart training...\")\n",
    "\n",
    "        for t in range(Config.num_epochs):\n",
    "            print(f\"\\nEpoch {t+1}\")\n",
    "            train_loss, train_acc = Iterator.epoch_iterator(train_dataloader, model, loss_fn, optimizer, multilabel=multilabel)\n",
    "            print(f\"Train loss: {train_loss:.3f} \\t Train acc: {train_acc:.3f}\")\n",
    "\n",
    "            val_loss, val_acc = Iterator.epoch_iterator(validation_dataloader, model, loss_fn, is_train=False, multilabel=multilabel)\n",
    "            print(f\"Val loss: {val_loss:.3f} \\t Val acc: {val_acc:.3f}\")\n",
    "\n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}\n",
    "                torch.save(save_dict, './pth_models/' + Config.model_name + '_best_model.pth')\n",
    "\n",
    "            # Save latest model\n",
    "            save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}\n",
    "            torch.save(save_dict, './pth_models/' + Config.model_name + '_latest_model.pth')\n",
    "\n",
    "            # Values for plotting\n",
    "            train_history[\"loss\"].append(train_loss)\n",
    "            train_history[\"accuracy\"].append(train_acc)\n",
    "            val_history[\"loss\"].append(val_loss)\n",
    "            val_history[\"accuracy\"].append(val_acc)\n",
    "        \n",
    "        print(\"Finished\")\n",
    "        return train_history, val_history\n",
    "\n",
    "    @staticmethod\n",
    "    def test(model, test_data, loss_function, multilabel=False):\n",
    "        # Load the best model (i.e. model with the lowest val loss...might not be the last model)\n",
    "        # We could also load the optimizer and resume training if needed\n",
    "        model = model.to(Config.device)\n",
    "        checkpoint = torch.load('./pth_models/' + Config.model_name + '_best_model.pth')\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "        test_loss, test_acc = Iterator.epoch_iterator(test_data, model, loss_function, is_train=False, multilabel=multilabel)\n",
    "        print(f\"\\nTest Loss: {test_loss:.3f} \\nTest Accuracy: {test_acc:.3f}\")\n",
    "\n",
    "        Utils.display_predictions(model, test_data, multilabel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Here we define the models of the Neural Networks to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Basic\n",
    "###################################################\n",
    "class ClassificationResNet:\n",
    "    def __init__(self, pre_trained = True):\n",
    "        self.pre_trained = pre_trained\n",
    "\n",
    "    def model(self):\n",
    "        model = models.resnet50(pretrained=self.pre_trained)\n",
    "        model.fc = nn.Linear(2048, 4)\n",
    "        model.to(Config.device)\n",
    "        return model\n",
    "\n",
    "    def run(self, train_dl, test_dl, validation_dl, loss_fn):\n",
    "        model = self.model()\n",
    "\n",
    "        train_history, val_history = Iterator.train(model, train_dl, validation_dl, loss_fn)\n",
    "        Utils.learning_curve_graph(train_history, val_history)\n",
    "\n",
    "        Iterator.test(model, test_dl, loss_fn)\n",
    "\n",
    "class ClassificationVGG16:\n",
    "    def __init__(self, pre_trained = True):\n",
    "        self.pre_trained = pre_trained\n",
    "\n",
    "    def model(self):\n",
    "        model = models.vgg16(pretrained=self.pre_trained)\n",
    "        model.classifier[6] = nn.Linear(4096, 4)\n",
    "        model.to(Config.device)\n",
    "        return model\n",
    "\n",
    "    def run(self, train_dl, test_dl, validation_dl, loss_fn):\n",
    "        model = self.model()\n",
    "\n",
    "        train_history, val_history = Iterator.train(model, train_dl, validation_dl, loss_fn)\n",
    "        Utils.learning_curve_graph(train_history, val_history)\n",
    "\n",
    "        Iterator.test(model, test_dl, loss_fn)\n",
    "\n",
    "###################################################\n",
    "# Intermediate\n",
    "###################################################\n",
    "class ClassificationCustomNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassificationCustomNetwork, self).__init__()\n",
    "        self.num_conv_layer = 2\n",
    "        self.num_max_pool = 1\n",
    "        output_size = Config.images_size\n",
    "        for _ in range(self.num_conv_layer):\n",
    "            output_size = Utils.calculate_output_size(output_size)\n",
    "        self.output_shape = (output_size, output_size, Config.num_filters)\n",
    "\n",
    "        for _ in range(self.num_max_pool):\n",
    "            self.output_shape = (self.output_shape[0]/Config.pool_size, self.output_shape[1]/Config.pool_size, self.output_shape[2])\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, Config.num_filters, Config.kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(Config.num_filters, Config.num_filters, Config.kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(Config.pool_size),\n",
    "\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(int(prod(self.output_shape)), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 4),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "class ClassificationCustomModel:\n",
    "    def __init__(self, pre_trained = True):\n",
    "        self.pre_trained = pre_trained\n",
    "\n",
    "    def model(self):\n",
    "        return ClassificationCustomNetwork().to(Config.device)\n",
    "\n",
    "    def run(self, train_dl, test_dl, validation_dl, loss_fn):\n",
    "        model = self.model()\n",
    "\n",
    "        train_history, val_history = Iterator.train(model, train_dl, validation_dl, loss_fn)\n",
    "        Utils.learning_curve_graph(train_history, val_history)\n",
    "\n",
    "        Iterator.test(model, test_dl, loss_fn)\n",
    "\n",
    "###################################################\n",
    "# Advanced - adapt the previous models to solve the original problem, i.e. multilabel classification, and compare their performance\n",
    "###################################################\n",
    "class ClassificationMultilabel:\n",
    "    def __init__(self, model_name, pre_trained = True):\n",
    "        self.model_name = model_name\n",
    "        self.pre_trained = pre_trained\n",
    "\n",
    "    def model(self):\n",
    "        if self.model_name == 'vgg16': return ClassificationVGG16().model()\n",
    "        elif self.model_name == 'resnet': return ClassificationResNet().model()\n",
    "        elif self.model_name == 'custom': return ClassificationCustomModel().model()\n",
    "        sys.exit('Invalid model')\n",
    "\n",
    "    def run(self, train_dl, test_dl, validation_dl, loss_fn): \n",
    "        model = ClassificationCustomNetwork().to(Config.device)\n",
    "\n",
    "        train_history, val_history = Iterator.train(model, train_dl, validation_dl, loss_fn, multilabel=True)\n",
    "        Utils.learning_curve_graph(train_history, val_history)\n",
    "\n",
    "        Iterator.test(model, test_dl, loss_fn, multilabel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "###################################################\n",
    "# Global Variables\n",
    "###################################################\n",
    "print(f\"Using {Config.device} device\\n\")\n",
    "\n",
    "###################################################\n",
    "# Transforms\n",
    "###################################################\n",
    "transforms_dict = {\n",
    "    \"train\": transforms.Compose([ \n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Resize((Config.images_size, Config.images_size)), \n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.RandomRotation(degrees=45),\n",
    "                    transforms.ToTensor()\n",
    "                ]),\n",
    "    \"validation\": transforms.Compose([\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Resize((Config.images_size, Config.images_size)), \n",
    "                    transforms.ToTensor()\n",
    "                ]),\n",
    "    \"test\": transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((Config.images_size, Config.images_size)),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "}\n",
    "\n",
    "###################################################\n",
    "# Read Images\n",
    "###################################################\n",
    "def read_images(filename):\n",
    "    images = []\n",
    "    with open(filename) as file:\n",
    "        while (line := file.readline().rstrip()):\n",
    "            images.append(line)\n",
    "    return images\n",
    "\n",
    "val_train_images = read_images('./data/train.txt')\n",
    "test_images = read_images('./data/test.txt')\n",
    "\n",
    "train_ratio = int(0.8 * len(val_train_images))\n",
    "validation_ratio = len(val_train_images) - train_ratio\n",
    "\n",
    "train_images = list(val_train_images[:train_ratio])\n",
    "validation_images = list(val_train_images[-validation_ratio:])\n",
    "\n",
    "###################################################\n",
    "# Models\n",
    "###################################################\n",
    "if __name__ == \"__main__\":\n",
    "    version = input('Enter the desired version (basic, intermediate, advanced): ')\n",
    "\n",
    "    if version == 'basic':\n",
    "        architecture = input('Choose the architecture (vgg16, resnet): ')\n",
    "        if architecture == 'vgg16':\n",
    "            neural_network = ClassificationVGG16(True)\n",
    "        elif architecture == 'resnet':\n",
    "            neural_network = ClassificationResNet(True)\n",
    "        else:\n",
    "            sys.exit('Invalid architecture')\n",
    "    elif version == 'intermediate':\n",
    "        neural_network = ClassificationCustomModel(True)\n",
    "    elif version == 'advanced':\n",
    "        model = input('Choose the model (vgg16, resnet, custom): ')\n",
    "        neural_network = ClassificationMultilabel(model, True)\n",
    "    else:\n",
    "        sys.exit('Invalid version')\n",
    "\n",
    "    if version == 'advanced':\n",
    "        train_data = ImageMultiLabelDataset(train_images, transforms_dict['train'])\n",
    "        validation_data = ImageMultiLabelDataset(validation_images, transforms_dict['validation'])\n",
    "        test_data = ImageMultiLabelDataset(test_images, transforms_dict['test'])\n",
    "    else:\n",
    "        train_data = ImageClassificationDataset(train_images, transforms_dict['train'])\n",
    "        validation_data = ImageClassificationDataset(validation_images, transforms_dict['validation'])\n",
    "        test_data = ImageClassificationDataset(test_images, transforms_dict['test'])\n",
    "\n",
    "    print(f'Training size: {len(train_data)}\\nValidation size: {len(validation_data)} \\nTest size: {len(test_data)}\\n')\n",
    "\n",
    "    labels_quantity = {'trafficlight': [], 'stop': [], 'speedlimit': [], 'crosswalk': []}\n",
    "    for image in train_data:\n",
    "        if version == 'advanced':\n",
    "            labels_idx = [i for i, x in enumerate(list(image['labels'])) if x == 1]\n",
    "            labels = [list(labels_quantity)[idx] for idx in labels_idx]\n",
    "            for label in labels:\n",
    "                labels_quantity[label].append(image)\n",
    "        else:\n",
    "            label = list(labels_quantity)[int(image['labels'].item())]\n",
    "            labels_quantity[label].append(image)\n",
    "\n",
    "    print('Labels quantity:')\n",
    "    for key, value in labels_quantity.items():\n",
    "        print(f'\\t{key}: {len(value)} images')\n",
    "\n",
    "    total_presences = sum([len(value) for value in labels_quantity.values()])\n",
    "    weights = [1 - len(value)/total_presences for value in labels_quantity.values()]\n",
    "\n",
    "    if version == 'advanced':\n",
    "        loss_fn = nn.BCELoss(weight = torch.tensor(weights, dtype=torch.float, device=Config.device))\n",
    "    else:\n",
    "        loss_fn = nn.CrossEntropyLoss(weight = torch.tensor(weights, dtype=torch.float, device=Config.device))\n",
    "\n",
    "    train_data = torch.utils.data.DataLoader(train_data, batch_size=Config.batch_size, shuffle=True, drop_last=True)\n",
    "    validation_data = torch.utils.data.DataLoader(validation_data, batch_size=Config.batch_size, shuffle=False, drop_last=False)\n",
    "    test_data = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True, drop_last=False)\n",
    "    \n",
    "    neural_network.run(train_data, test_data, validation_data, loss_fn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
